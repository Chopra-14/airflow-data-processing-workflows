# ğŸš€ Build Data Processing Workflows with Apache Airflow and Docker

## ğŸ“Œ Project Overview
This project demonstrates a complete, production-style **data engineering workflow** using **Apache Airflow**, **Docker**, and **PostgreSQL**.  
It showcases how to orchestrate ETL pipelines, apply data transformations, export analytics-ready data, implement conditional logic, handle failures, and validate workflows using unit tests.

The project consists of **five distinct DAGs**, each highlighting a different workflow orchestration pattern commonly used in real-world data engineering systems.

---

## ğŸ—ï¸ Architecture Overview

**Tech Stack:**
- Apache Airflow 2.8
- Docker & Docker Compose
- PostgreSQL (metadata store + warehouse)
- Pandas & PyArrow
- Pytest (unit testing)

**Architecture Flow:**

CSV â†’ PostgreSQL â†’ Transformed PostgreSQL â†’ Parquet  
                     â†˜ Conditional Logic  
                     â†˜ Notifications & Error Handling  

All services run inside Docker containers for reproducibility and isolation.

---

## ğŸ“‚ Project Structure

airflow-data-pipeline/
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ dag1_csv_to_postgres.py
â”‚ â”œâ”€â”€ dag2_data_transformation.py
â”‚ â”œâ”€â”€ dag3_postgres_to_parquet.py
â”‚ â”œâ”€â”€ dag4_conditional_workflow.py
â”‚ â””â”€â”€ dag5_notification_workflow.py
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_dag1.py
â”‚ â”œâ”€â”€ test_dag2.py
â”‚ â””â”€â”€ test_utils.py
â”œâ”€â”€ data/
â”‚ â””â”€â”€ input.csv
â”œâ”€â”€ output/
â”‚ â””â”€â”€ employee_data_YYYY-MM-DD.parquet
â”œâ”€â”€ logs/
â”œâ”€â”€ plugins/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

yaml
Copy code

---

## âš™ï¸ Prerequisites

Make sure the following are installed:
- Docker
- Docker Compose
- Git

---

## ğŸ³ Setup Instructions (Docker)

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/Chopra-14/airflow-data-processing-workflows.git
cd airflow-data-processing-workflows
2ï¸âƒ£ Start Airflow Environment
bash
Copy code
docker compose up -d
ğŸŒ Access Airflow UI
URL: http://localhost:8080

Username: admin

Password: admin

ğŸ”„ DAG Descriptions & Execution
ğŸŸ¢ DAG 1 â€” CSV to PostgreSQL Ingestion
DAG ID: csv_to_postgres_ingestion

Creates raw_employee_data table

Truncates table (idempotent)

Loads CSV data from data/input.csv

Trigger:
Enable â†’ Trigger DAG
Expected Output:

Table populated with 100 rows

ğŸŸ¢ DAG 2 â€” Data Transformation Pipeline
DAG ID: data_transformation_pipeline

Transformations:

full_info = name + city

age_group = Young / Mid / Senior

salary_category = Low / Medium / High

year_joined extracted from join_date

Expected Output:

transformed_employee_data table with transformed columns

ğŸŸ¢ DAG 3 â€” PostgreSQL to Parquet Export
DAG ID: postgres_to_parquet_export

Checks source table

Exports data to Parquet using pyarrow + snappy

Validates file schema

Expected Output:

Parquet file created in output/ directory
Example:

Copy code
employee_data_2026-01-03.parquet
ğŸŸ¢ DAG 4 â€” Conditional Workflow
DAG ID: conditional_workflow_pipeline

Branching Logic:

Day	Branch
Monâ€“Wed	Weekday
Thuâ€“Fri	End-of-week
Satâ€“Sun	Weekend

Uses BranchPythonOperator

Only one branch runs per execution

End task always runs

ğŸŸ¢ DAG 5 â€” Notifications & Error Handling
DAG ID: notification_workflow

Logic:

Task fails if execution day % 5 == 0

Success & failure callbacks

Cleanup task runs always

Expected Behavior:

Success days â†’ success notification

Failure days â†’ failure notification

Cleanup always executes

ğŸ§ª Running Unit Tests
Unit tests validate DAG structure without running Airflow or connecting to databases.

Run tests inside Docker container:
bash
Copy code
docker exec -it airflow_webserver bash
pip install pytest
pytest /opt/airflow/tests -v
Expected Result:
Copy code
12 passed, 0 failed
ğŸ› ï¸ Troubleshooting
âŒ DAG not visible?
Ensure file is inside dags/

Check Browse â†’ DAG Import Errors

Restart Airflow:

bash
Copy code
docker compose restart airflow-scheduler airflow-webserver
âŒ PostgreSQL connection error?
Verify Airflow connection:

Connection ID: postgres_default

Host: postgres

Schema: airflow_db

User: airflow_user

Password: airflow_pass

âŒ Pytest not found?
Install inside container:

bash
Copy code
pip install pytest
âœ… Conclusion
This project demonstrates:

End-to-end ETL orchestration

Data transformation best practices

Analytics-ready data export

Conditional workflows

Error handling & notifications

Professional unit testing

It reflects real-world data engineering standards using Apache Airflow.

